{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e588890-60e1-480c-88b5-14347762247a",
   "metadata": {},
   "source": [
    "## Домашнее задание №1\n",
    "\n",
    "### Предлагается реализовать поисковую систему, способную работать с proximity-оператором в запросе\n",
    "\n",
    "#### Ограничения:\n",
    "- В запросах встречаются только операторы OW/N и UW/N, где N - кол-во токенов, которое учитывается операторами\n",
    "- Система должна уметь обрабатывать:\n",
    "  - однотокенные запросы (без proximity-оператора), например: \"молоко\"\n",
    "  - двухтокенные запросы с proximity-оператором между токенами, например: \"молочный OW/1 ломтик\"\n",
    "- Система должна возвращать в ответе максимум 20 товаров\n",
    "- Весь код должен быть абсолютно воспроизводимым - в режиме \"Run all\"\n",
    "\n",
    "#### Цель - получить работающую поисковую систему с метриками качества на валидационном наборе запросов (прилагается к заданию):\n",
    "- precision > 0.3\n",
    "- recall > 0.4\n",
    "\n",
    "#### Разбалловка за каждый пункт указана ниже, максимальное количество баллов за задание - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9e0c6-dca3-4a45-b8f6-4a5f71e2ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9d805-62b8-4987-ba6b-b5d6721f20d5",
   "metadata": {},
   "source": [
    "### Дан корпус документов - база товаров с их именами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace129a-0a16-4e4d-871b-3b5332ed4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(\"products_with_names.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfefb46-979a-4016-9a02-ffc7b1c701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dict = {\n",
    "    doc[1][\"product_id\"]: doc[1][\"name\"] for doc in dataset.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cccb7-d5ae-4db2-9b2c-204ef80db443",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: int\n",
    "    name: str\n",
    "\n",
    "\n",
    "documents = [Document(doc_id=doc[1][\"product_id\"], name=doc[1][\"name\"]) for doc in dataset.iterrows()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff42a5-be07-4ecd-acca-02f400b1b7e5",
   "metadata": {},
   "source": [
    "### Пример реализации класса обработчика текста\n",
    "\n",
    "Не обязательно использовать его в таком виде, можно реализовать любую обработку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ab044-3477-4894-a405-eb5d3e67acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.symbols_to_replace = {\"ё\": \"е\"}\n",
    "        self.stopwords = set(stopwords.words(\"russian\"))\n",
    "        self.linguist = MorphAnalyzer()\n",
    "\n",
    "    def lowercase_text(self, text: str) -> str:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "\n",
    "    def replace_symbols(self, text: str) -> str:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "\n",
    "    def process_punctuation_simple(self, text: str) -> str:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "\n",
    "    def tokenize_simple(self, text: str) -> list[str]:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "\n",
    "    def remove_stopwords(self, doc: list[str]) -> list[str]:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "\n",
    "    def lemmatize_token(self, token: str) -> str:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "    \n",
    "    def lemmatize_tokenized_text(self, tokenized_text: list[str]) -> list[str]:\n",
    "        # TODO: add some code here\n",
    "        pass\n",
    "\n",
    "    def process_text(self, text: str) -> list[str]:\n",
    "        text = self.lowercase_text(text)\n",
    "        text = self.replace_symbols(text)\n",
    "        text = self.process_punctuation_simple(text)\n",
    "        text_tokens = self.tokenize_simple(text)\n",
    "        text_tokens = self.remove_stopwords(text_tokens)\n",
    "        return self.lemmatize_tokenized_text(text_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3564256-1465-4d30-9928-d8acb036021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68746ac7-8d99-4c23-a83e-19c4e935aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessedDocument:\n",
    "    doc_id: int\n",
    "    name_tokens: list[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05e409-7f07-4a4f-8a24-e8895cbc9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_processed = [\n",
    "    ProcessedDocument(\n",
    "        doc_id=document.doc_id, \n",
    "        name_tokens=text_processor.process_text(document.name),\n",
    "    ) \n",
    "    for document in tqdm(documents)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adb0f6-0fe5-47a9-8529-d340fa78a794",
   "metadata": {},
   "source": [
    "### (2 балла) Построение positional inverted index - обратного индекса с токенопозициями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008f58d-3dd5-45a5-949e-1f45e87fa74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocIdWithTokenPositions:\n",
    "    doc_id: int\n",
    "    token_positions: list[int]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858ecf4-116a-4c92-8f6a-1bfaec4156f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_inverted_index(\n",
    "    documents_processed: list[ProcessedDocument],\n",
    "    limit: Optional[int] = None,\n",
    ") -> dict[str, list[DocIdWithTokenPositions]]:\n",
    "    \"\"\"\n",
    "    Функция построения обратного индекса с токенопозициями\n",
    "\n",
    "    Args:\n",
    "        documents_processed (list[ProcessedDocument]): Набор документов корпуса с именами, прошедшими лингвистическую обработку\n",
    "        limit (int, optional): Параметр для ограничения кол-ва документов для построения индекса\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[DocIdWithTokenPositions]]: hash-based обратный индекс с токенопозициями\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: add some code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8841a-6ed0-4e21-90f3-75f6eb2a434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_inverted_index = create_positional_inverted_index(documents_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef59dd-bc3d-40d0-9eb0-0943d875319e",
   "metadata": {},
   "source": [
    "### (3 балла) Реализация функции обработки запроса и слияния списков документов, учитывая позиции токенов в документах и proximity-оператор в запросе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c278dd0-4f99-448f-959e-923f35d409ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_posting_lists_with_condition(\n",
    "    posting_lists: tuple[DocIdWithTokenPositions, DocIdWithTokenPositions], \n",
    "    condition: Literal[\"OW/1\", \"OW/2\", \"OW/3\", \"UW/1\",\"UW/2\",\"UW/3\"],\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Функция реализует слияние двух списков документов с условием proximity-оператора\n",
    "\n",
    "    Args:\n",
    "        posting_lists (tuple[DocIdWithTokenPositions, DocIdWithTokenPositions]): Кортеж из двух списков документов с токенопозициями\n",
    "        condition (Literal[\"OW/1\", \"OW/2\", \"OW/3\", \"UW/1\",\"UW/2\",\"UW/3\"]): Proximity-оператор\n",
    "\n",
    "    Returns:\n",
    "        list[int]: Результат слияния списков документов\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: add some code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623b5ed-29fc-4981-a501-1140ad3753c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_over_positional_inverted_index(\n",
    "    positional_inverted_index: dict[str, list[DocIdWithTokenPositions]], \n",
    "    documents_dict: dict[int, str],\n",
    "    query: str,\n",
    "    limit: Optional[int] = None,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Функция реализует поиск по обратному индексу\n",
    "\n",
    "    Args:\n",
    "        positional_inverted_index (dict[str, list[DocIdWithTokenPositions]]): Обратный индекс с токенопозициями\n",
    "        documents_dict (dict[int, str]): Корпус документов\n",
    "        query (str): Запрос пользователя\n",
    "        limit (int, optional): Параметр, ограничивающий количество возвращаемых документов\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: Результат поиска\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: add some code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ae7fb-0f1d-4245-abf7-46ceb6fb3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_over_positional_inverted_index(positional_inverted_index, documents_dict, \"картофель\")[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafba6c-7ad2-4426-8d0b-b873133a1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_over_positional_inverted_index(positional_inverted_index, documents_dict, \"молочный OW/1 ломтик\")[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac65b8-e105-4dd6-af82-2faad14954a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_over_positional_inverted_index(positional_inverted_index, documents_dict, \"уксус UW/2 яблочный\")[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2263159-238c-4f1a-b1ee-76b70db1ead3",
   "metadata": {},
   "source": [
    "### Читаем валидационный набор запросов с позитивными примерами товаров\n",
    "\n",
    "Метрики будут считаться на нем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1250b-e35a-44e1-8f57-f1cd37ec040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proximity_validation_query_positives = pd.read_parquet(\"result_proximity_validation_query_positives.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e93f3-53ee-4386-86d7-a49ac736ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proximity_validation_query_positives_dict = {\n",
    "    row[1].query: row[1].products.tolist()\n",
    "    for row in result_proximity_validation_query_positives.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e3f84-0585-471e-87e1-69d09f4dc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_list = []\n",
    "search_results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f327cd-a894-445b-b2da-71d03fa3dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query, ground_truth_products in result_proximity_validation_query_positives_dict.items():\n",
    "    ground_truths_list.append(ground_truth_products)\n",
    "    search_results_list.append(\n",
    "        search_over_positional_inverted_index(positional_inverted_index, documents_dict, query, limit=20)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea309ac0-8633-43f7-b537-c00ffaca1aa1",
   "metadata": {},
   "source": [
    "### (3 балла - за соответствие требованиям к метрикам) Посчитаем метрики качества системы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30081490-3b9f-490f-add8-ad4c20eace72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Metrics:\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"precision = {self.precision}\\nrecall = {self.recall}\\nf1_score = {self.f1_score}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec77c6-a025-4e47-92e8-f556270e97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(ground_truth_set: set[int], search_results_set: set[int]):\n",
    "    \n",
    "    tp = len(ground_truth_set.intersection(search_results_set))\n",
    "    \n",
    "    precision = tp / len(search_results_set) if len(search_results_set) > 0 else 0.0\n",
    "    \n",
    "    recall = tp / len(ground_truth_set) if len(ground_truth_set) > 0 else 0.0\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return Metrics(precision=precision, recall=recall, f1_score=f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b2884-2646-4d32-b798-9e509867bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_metrics(ground_truth_products_lists, search_products_lists):\n",
    "    metrics = []\n",
    "    for ground_truth, search in zip(ground_truth_products_lists, search_products_lists):\n",
    "        metrics.append(\n",
    "            calculate_metrics(set(ground_truth), set(x.doc_id for x in search))\n",
    "        )\n",
    "    \n",
    "    return Metrics(\n",
    "        precision=np.mean([x.precision for x in metrics]),\n",
    "        recall=np.mean([x.recall for x in metrics]),\n",
    "        f1_score=np.mean([x.f1_score for x in metrics]),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644ff6a-e8c2-47b1-82bc-f86106f91e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_validation_metrics(ground_truths_list, search_results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df23db-7640-4679-a1c3-0627b0fadc98",
   "metadata": {},
   "source": [
    "### (2 балла) Сравнение метрик с поиском по обратному индексу без учета токенопозиций\n",
    "\n",
    "Нужно:\n",
    "- реализовать поиск над обратным индексом без токенопозиций;\n",
    "- посчитать метрики качества (precision, recall) на том же валидационном наборе запросов (игнорируя proximity-операторы);\n",
    "- сравнить метрики с первым подходом и сделать вывод о полезности токенопозиций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db838598-1162-462d-9b8c-056b7f12a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e9c0a-10d0-46fd-af3e-9926930c04bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
